# Linguistic Analysis Service Documentation (`linguistic_service.py`)

This document provides an overview of the functions within `backend/services/linguistic_service.py`. This module offers various quantitative linguistic analyses of transcribed text, focusing on patterns that can support deception detection and behavioral analysis. It does not contain a class named `LinguisticService` but rather a collection of related functions.

---

## Function: `analyze_linguistic_patterns(transcript: str, duration: float = None) -> Dict[str, Any]`

### Purpose
This is the core function for analyzing various linguistic patterns in a given transcript. It calculates a range of quantitative metrics related to word usage, hesitation, formality, complexity, and more.

### Input Parameters
*   **`transcript: str`**: The transcribed text to be analyzed.
*   **`duration: float`** (Default: `None`): Optional duration of the audio in seconds. If provided, it's used to calculate rate-based metrics like speech rate (WPM) and hesitation rate.

### Return Value
*   **`Dict[str, Any]`**: A dictionary containing numerous linguistic metrics. Key metrics include:
    *   `"word_count"`: Total number of words.
    *   `"hesitation_count"`: Count of hesitation words (e.g., "um", "uh", "like", "you know").
    *   `"qualifier_count"`: Count of qualifier words suggesting uncertainty (e.g., "maybe", "perhaps", "i think").
    *   `"certainty_count"`: Count of words indicating certainty (e.g., "definitely", "absolutely", "know").
    *   `"filler_count"`: Count of simple filler words (e.g., "um", "uh").
    *   `"repetition_count"`: Count of immediate word repetitions and short phrase repetitions.
    *   `"formality_score"`: A score from 0-100 indicating the perceived formality of the text. (See "Formality Score Calculation" below).
    *   `"complexity_score"`: A score from 0-100 indicating linguistic complexity. (See "Complexity Score Calculation" below).
    *   `"avg_word_length"`: Average length of words.
    *   `"avg_words_per_sentence"`: Average number of words per sentence.
    *   `"sentence_count"`: Total number of sentences.
    *   `"speech_rate_wpm"`: Words per minute (if `duration` is provided).
    *   `"hesitation_rate"`: Hesitations per minute (if `duration` is provided).
    *   `"confidence_ratio"`: Ratio of certainty words to the sum of qualifier and certainty words.
    *   It also includes descriptive text fields generated by helper functions (e.g., `"speech_patterns"`, `"word_choice"`), though these are primarily for backward compatibility as noted in `linguistic_analysis_pipeline`.
    *   Returns a default dictionary from `get_default_linguistic_analysis()` if the transcript is empty or an error occurs.

### Key Logic & Algorithms
*   **Text Statistics**: Basic counts of words and sentences.
*   **Pattern Matching**: Uses regular expressions (`re.findall`) with `re.IGNORECASE` to find occurrences of various word categories (hesitations, qualifiers, certainty, formal, informal, fillers).
*   **Repetition Detection**:
    *   Immediate repetitions: `\b(\w+)\s+\1\b`.
    *   Phrase repetitions: Iteratively checks for sequences of 2-4 words that reappear later in the text.
*   **Formality Score Calculation**:
    *   Identifies an extensive list of formal words (transitions, courtesy, legal/business, academic, expressions) and informal words (casual responses, slang, standard contractions).
    *   Calculates ratios of formal words, casual informal words, and standard informal words to the total word count.
    *   The score is derived using a baseline of 50, adding a weighted `formal_ratio` (multiplied by 500) and subtracting weighted penalties for `casual_penalty` (multiplied by 250) and `standard_penalty` (multiplied by 100). The result is clamped between 0 and 100.
    *   This is an enhanced approach compared to the one described in `memory/FORMALITY_SCORING_DOCUMENTATION.md`.
*   **Complexity Score Calculation**:
    *   Calculated based on several factors:
        1.  `word_length_factor`: Average word length scaled (multiplied by 15).
        2.  `sentence_length_factor`: Average words per sentence scaled (multiplied by 3).
        3.  `vocabulary_diversity`: Ratio of unique words to total words, as a percentage.
        4.  `certainty_balance`: Penalty based on the absolute difference between certainty and qualifier counts.
    *   A `complexity_base` is the average of these factors.
    *   A `hesitation_penalty` (scaled ratio of hesitations to words) is subtracted from the `complexity_base`.
    *   The final score is clamped between 0 and 100.
*   **Error Handling**: If any exception occurs during analysis, it logs the error and returns default values via `get_default_linguistic_analysis()`.

---

## Function: `generate_speech_patterns_description(word_count: int, hesitation_count: int, speech_rate_wpm: float = None, complexity_score: float = 50) -> str`

### Purpose
Generates a descriptive string summarizing speech patterns based on provided metrics.

### Input Parameters
*   `word_count: int`
*   `hesitation_count: int`
*   `speech_rate_wpm: float` (Optional)
*   `complexity_score: float` (Default: 50)

### Return Value
*   `str`: A descriptive summary string.

---

## Function: `generate_word_choice_description(avg_word_length: float, formality_score: float, qualifier_count: int, certainty_count: int) -> str`

### Purpose
Generates a descriptive string summarizing word choice patterns.

### Input Parameters
*   `avg_word_length: float`
*   `formality_score: float`
*   `qualifier_count: int`
*   `certainty_count: int`

### Return Value
*   `str`: A descriptive summary string.

---

## Function: `generate_emotional_consistency_description(hesitation_count: int, qualifier_count: int, confidence_ratio: float) -> str`

### Purpose
Generates a descriptive string for emotional consistency based on hesitation, qualifiers, and confidence ratio.

### Input Parameters
*   `hesitation_count: int`
*   `qualifier_count: int`
*   `confidence_ratio: float`

### Return Value
*   `str`: A descriptive summary string.

---

## Function: `generate_detail_level_description(avg_words_per_sentence: float, complexity_score: float, word_count: int) -> str`

### Purpose
Generates a descriptive string for the level of detail in the speech.

### Input Parameters
*   `avg_words_per_sentence: float`
*   `complexity_score: float`
*   `word_count: int`

### Return Value
*   `str`: A descriptive summary string.

---

## Function: `get_default_linguistic_analysis() -> Dict[str, Any]`

### Purpose
Provides a default dictionary structure for linguistic analysis results, typically used in error cases or when a transcript is empty.

### Return Value
*   `Dict[str, Any]`: A dictionary with all linguistic metric keys initialized to default values (0, None, or "Analysis unavailable").

---

## Function: `linguistic_analysis_pipeline(transcript: str, duration: float = None) -> Dict[str, Any]`

### Purpose
This function acts as a pipeline to perform linguistic analysis. It primarily calls `analyze_linguistic_patterns` and then uses its results to generate several descriptive summaries.

### Input Parameters
*   **`transcript: str`**: The transcribed text.
*   **`duration: float`** (Default: `None`): Optional audio duration in seconds.

### Return Value
*   **`Dict[str, Any]`**:
    *   Returns the dictionary from `analyze_linguistic_patterns`. The initially generated descriptive strings within this dictionary seem to be what's returned, though the code shows calls to `generate_..._description` functions whose return values are assigned to local variables but not explicitly merged back into the `linguistic_analysis` dict before returning. This might be an oversight if the intent was to update them. (Correction: The `analyze_linguistic_patterns` function itself calls these generation functions and includes their output in its returned dictionary, so `linguistic_analysis_pipeline` directly returns the comprehensive dictionary from `analyze_linguistic_patterns`.)

### Key Operations
1.  Calls `analyze_linguistic_patterns(transcript, duration)` to get the core metrics.
2.  Calls the four `generate_..._description` helper functions using metrics from the `linguistic_analysis` result.
3.  Returns the `linguistic_analysis` dictionary which already contains these descriptions from its own internal calls.
4.  Includes a `try-except` block to log errors if any part of the pipeline fails.

---
