# DSPyTranscriptionModule Module Documentation

## Purpose

The `DSPyTranscriptionModule` is a specialized DSPy module designed to transcribe audio data into text. It takes a base64 encoded audio string and its MIME type as input and returns the transcribed text. This module is intended for use with multimodal Language Models (LMs) that are capable of processing audio data.

## TranscriptionSignature

The `DSPyTranscriptionModule` operates based on the `TranscriptionSignature`. This signature defines the specific input fields required for the audio data and the expected output field for the transcript.

### Input Fields

The signature specifies the following input fields, which are crucial for the LM to process the audio:

*   **`audio_base64_string`**:
    *   **Type**: `str`
    *   **Description**: A base64 encoded string representing the raw audio data to be transcribed.
*   **`audio_mime_type`**:
    *   **Type**: `str`
    *   **Description**: The MIME type of the audio data (e.g., 'audio/wav', 'audio/mp3', 'audio/webm'). This helps the LM correctly interpret the audio format.
*   **`context_prompt`** (Optional, as per current `TranscriptionSignature`):
    *   **Type**: `str`
    *   **Description**: An optional field for providing context or specific instructions for the transcription process. (Note: The example `TranscriptionSignature` in `dspy_modules.py` shows this commented out, but it's a common pattern if context is needed).

### Output Fields

The signature defines the following output field:

*   **`transcript_text`**:
    *   **Type**: `str`
    *   **Description**: The transcribed text generated by the LM from the provided audio data.

## Usage

The `DSPyTranscriptionModule` is primarily used within services that handle audio processing, such as the `dspy_transcribe_audio` function in `backend/services/core_dspy_services.py`.

The typical workflow is as follows:
1.  An audio file path is provided to a service function (e.g., `dspy_transcribe_audio`).
2.  The service function reads the audio file, encodes it into a base64 string, and determines its MIME type.
3.  An instance of `DSPyTranscriptionModule` is created.
4.  The `forward` method of the module is called with the `audio_base64_string` and `audio_mime_type`.
5.  Unlike other analyzer modules that often use `dspy.ChainOfThought`, the `DSPyTranscriptionModule` uses `dspy.Predict(TranscriptionSignature)`. This is because transcription is generally a more direct task that doesn't require extensive chain-of-thought reasoning.
6.  The `forward` method returns the transcribed text as a string. If transcription fails or the LM returns no text, a default message like "Transcription failed or returned no text." may be returned.
7.  The service function (`dspy_transcribe_audio`) may include additional logic, such as ensuring the DSPy LM is configured (e.g., by calling `GeminiService()` if no LM is set) and handling file operations and exceptions. It often runs the synchronous `forward` method of the DSPy module in a separate thread using `asyncio.to_thread` to avoid blocking asynchronous operations.

**Note on LM Configuration:** The effective use of this module heavily depends on the correct configuration of the underlying `dspy.LM` (e.g., `GeminiService` for Gemini models) to handle multimodal inputs. The `dspy.LM` wrapper is responsible for constructing the request that the multimodal model API (like Gemini's) understands, including how it handles the base64 audio string and MIME type.

## Underlying DSPy Signature: `TranscriptionSignature`

The `TranscriptionSignature` (a class inheriting from `dspy.Signature`) defines the specific structure for the transcription task.

*   **Purpose**: It clearly outlines to the LM what kind of input it will receive (base64 audio and its MIME type) and what output it should produce (the transcript text). The docstring of the signature provides general instructions like handling multiple speakers and preserving natural speech flow.
*   **Mechanism**: It uses `dspy.InputField` for `audio_base64_string` and `audio_mime_type`, and `dspy.OutputField` for `transcript_text`. The descriptions (`desc`) for these fields guide the LM.
*   **Benefit**: This structured approach allows DSPy to manage the interaction with the multimodal LM for transcription. It standardizes how audio data is presented to the LM for this task. The directness of `dspy.Predict` is suitable for transcription's nature as a generative task based on direct audio input.
